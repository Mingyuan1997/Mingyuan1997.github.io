<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Mingyuan Wu Personal Website</title>
  <style>
    :root{
      --bg: #fbf6ed;
      --paper: #fffaf1;
      --ink: #1f2328;
      --muted: #6b7280;
      --accent: #0a58ca;
      --rule: #e8e0d1;
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family: ui-serif, Georgia, Cambria, "Times New Roman", Times, serif;
      color:var(--ink);
      background:var(--bg);
      line-height:1.55;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    .page{
      max-width:1100px;
      margin:0 auto;
      padding:24px 20px 80px;
      display:grid;
      grid-template-columns: 280px 1fr;
      grid-gap: 32px;
    }
    @media (max-width: 860px){
      /* Keep two-column layout but make sidebar smaller */
      .page{
        grid-template-columns: 180px 1fr;  /* Smaller sidebar on mobile */
        grid-gap: 16px;  /* Tighter spacing on mobile */
        padding: 16px 12px 60px;
      }
      .sidebar{
        position: sticky;  /* Keep sticky behavior */
        top: 16px;
        padding: 12px;  /* Less padding */
      }
      /* Scale down sidebar elements */
      .avatar{
        width: 80px;
        height: 80px;
        border-radius: 10px;
      }
      .name{
        font-size: 1.1rem;  /* Smaller name */
      }
      .name small{
        font-size: .8rem;
      }
      .links{
        font-size: 0.8rem;
        gap: 6px 10px;
      }
      .misc{
        font-size: .8rem;
      }
      .misc .label{
        font-size: .85rem;
      }
      .misc ul{
        padding-left: 0.9rem;
      }
      /* Adjust main content padding */
      .content{
        padding: 16px 18px;
      }
    }
    .card{
      background:var(--paper);
      border:1px solid var(--rule);
      border-radius:14px;
      box-shadow: 0 1px 0 rgba(0,0,0,.02), 0 6px 20px rgba(0,0,0,.04);
    }
    .sidebar{
      position:sticky;
      top:16px;
      padding:18px;
    }
    .profile{
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:14px;
    }
  
    /* Smaller misc text in sidebar */
    .misc{
      font-size:.9rem;          /* smaller than body (1rem) */
      line-height:1.4;
      color:var(--muted);
      margin-top:8px;
    }
    .misc .label{
      display:block;
      font-weight:700;
      font-size:.95rem;         /* small label, not as big as .name */
      color:var(--ink);
      margin-bottom:4px;
    }
    .misc p{ margin:6px 0 0; }
    /* tidy list spacing for misc-style lists */
    .misc ul{ margin:6px 0 0; padding-left:1.05rem; }
    .misc li{ margin:2px 0; }

    .avatar{
      width:140px;height:140px;border-radius:14px;
      object-fit:cover;
      border:1px solid var(--rule);
      background:#f1eadf;
      display:block;
    }
    .name{
      font-weight:700;
      font-size:1.4rem;
    }
    .name small{display:block;color:var(--muted);font-size:.95rem;margin-top:2px}
    .links{
      display:flex;flex-wrap:wrap;gap:10px 14px;justify-content:center;margin-top:6px;
      font-size:0.95rem;
    }
    .research #pub-list .publication-item .links{
      display: flex;
      gap: .5rem;
      justify-content: flex-start;
      text-align: left;
      margin-left: 0;
    }
    .links a{position:relative}
    .links a:after{
      content:"";position:absolute;left:0;bottom:-1px;width:100%;height:1px;background:currentColor;opacity:.15;
      transform:scaleX(0);transform-origin:left;transition:transform .2s ease;
    }
    .links a:hover:after{transform:scaleX(1)}
    .anon-feedback{display:block;text-align:center;margin-top:2px}
    .content{
      padding:22px 26px;
    }
    h1,h2{
      margin: 10px 0 12px;
    }
    h2{
      font-size:1.15rem;
      letter-spacing:.02em;
      text-transform:uppercase;
      font-weight:800;
      border-bottom:1px solid var(--rule);
      padding-bottom:6px;
    }
    .hello h1, .hello h2{
      font-size:1.6rem;
      display:flex;align-items:center;gap:.55rem;
    }
    .hello p{margin: 10px 0}
    .muted{color:var(--muted)}
    .select-line{
      font-size:.95rem;color:var(--muted);
    }
    .list{
      list-style:none;margin:0;padding:0;
    }
    .list li{
      padding:10px 0 12px;
      border-bottom:1px dashed var(--rule);
    }
    .paper-title{
      font-weight:700;
    }
    .paper-meta{
      color:var(--muted);
      font-size:.96rem;
      margin-top:2px;
    }
    .emoji{font-size:1.1em}
    .events .date{
      font-weight:700;
    }
    .foot-space{height:16px}
    .chip{
      display:inline-block;
      font-size:.8rem;
      padding:.1rem .45rem;
      border:1px solid var(--rule);
      border-radius:999px;
      background:#fff;
      vertical-align:baseline;
      margin-left:.4rem;
      color:var(--muted);
    }
    .topbar{
      display:flex;justify-content:flex-end;gap:10px;margin-bottom:10px;color:var(--muted);font-size:.92rem;
    }

    /* === UI: view toggles & footnotes === */
    .view-toggle{display:inline-flex;gap:.35rem;align-items:center;font-size:.95rem}
    .toggle-link{background:none;border:0;padding:0 .1rem;font:inherit;cursor:pointer;color:var(--accent)}
    .toggle-link:hover{text-decoration:underline}
    .toggle-link.active{font-weight:700;text-decoration:underline}

    .footnote-trigger{cursor:pointer}
    .footnote-popup{position:absolute;z-index:9999;max-width:320px;background:var(--paper);border:1px solid var(--rule);border-radius:10px;box-shadow:0 10px 25px rgba(0,0,0,.12);opacity:0;transform:translateY(-4px);transition:opacity .18s ease, transform .18s ease}
    .footnote-popup.active{opacity:1;transform:translateY(0)}
    .footnote-content{padding:10px 12px;position:relative}
    .footnote-close{position:absolute;top:6px;right:6px;background:transparent;border:0;font-size:1rem;cursor:pointer}

    /* Optional: expand details on mobile when item has .expanded */
    .publication-item.expanded .details{display:block}
  </style>
</head>
<body>
  <div class="page">
    <!-- Left: Sidebar -->
    <aside class="sidebar card">
      <div class="profile">
        <img class="avatar" alt="Pic of Mingyuan Wu"
             src="assets/image_mingyuan.jpg"
             width="1000" height="1000" loading="lazy" decoding="async">
        <div class="name">Mingyuan Wu</div>
        <div class="name"><small>ÊòéËøú</small></div>
        <div class="links">
          <a href="https://x.com/MingyuanWu4" rel="noopener">X/Twitter</a>
          <a href="https://github.com/Mingyuan1997" rel="noopener">GitHub</a>
          <a href="https://scholar.google.com/citations?user=kp3PK7IAAAAJ&hl=en" rel="noopener">Google Scholar</a>
          <a href="mailto:mw34@illinois.edu" rel="noopener" class="footnote-trigger" data-footnote-content="mw34@illinois.edu">Email</a>
        </div>    
        <div class="name">Under Construction</div>

        <div class="misc">
          <span class="label">Misc</span>
          <p>When I was younger, I was a fan of the <a href="https://en.wikipedia.org/wiki/Steins;Gate_(TV_series)">anime Steins;Gate</a> and dreamed of building personal agents like the Amadeus system.</p>
        </div>

        <!-- NEW: Mentees (same size/style as .misc) -->
        <div class="misc">
          <span class="label">Research mentees and friends</span>
          <ul>
            <!-- Replace with real names + links -->
             <li><a href="https://jingcheng.me/" rel="noopener">Jingcheng Yang</a>, UIUC Undergraduate (CRA candidate) ‚Üí </li>
            <li><a href="https://openreview.net/profile?id=~Haozhen_Zheng1" rel="noopener">Haozhen Zheng</a>, UIUC Undergraduate ‚Üí UIUC CS PhD program</li>
            <li><a href="https://www.linkedin.com/in/revanjrf/" rel="noopener">Revan Ji</a>, UIUC Undergraduate ‚Üí UIUC ECE MS/PhD program</li>
            <li><a href="https://www.linkedin.com/in/jize-jiang-a61627188/" rel="noopener">Jize Jiang</a>, UIUC Undergraduate ‚Üí UIUC CS PhD program</li>
            <li><a href="https://billyzhaohengli.github.io/" rel="noopener">Billy Li</a>, UIUC Undergraduate ‚Üí UIUC CS PhD program</li>
            <li><a href="https://shivvtrivedi.com/" rel="noopener">Shiv Trivedi</a>, UIUC Undergraduate (CRA candidate) -> Georgia Tech CS PhD program</li>
            <li><a href="https://www.linkedin.com/in/meitang-li-94a231250/" rel="noopener">Meitang Li</a>, Umich Master -> Max and Cynthia LLC</li>
            <li><a href="#" rel="noopener">Yuhan Lu</a>, UIUC Undergraduate ‚Üí UIUC MSCS program/Tiktok</li>
          </ul>
        </div>
        <div class="misc">
          <span class="label">Research works in progress (preprint not yet posted)</span>
          <p>SVR-R1: Bootstrapping Multi-modal Reasoning with Self-verification in Reinforcement Learning</p>
          <p>Circuit Tracing in Vision‚ÄìLanguage Models: Understanding the Internal Mechanisms of Multimodal Thinking</p>        
        </div>

      </div>
    </aside>

    <!-- Right: Main content -->
    <main class="content card">
      <div class="hello">
        <h2>Some Random Stuff <span class="emoji">üëã</span></h2>
        <p>I am Mingyuan Wu, a part-time researcher at <a href="https://ai.meta.com/">Meta</a>. I am currently a final-year PhD candidate in Computer Science at <a href="https://illinois.edu/"> University of Illinois, Urbana Champaign</a>. I am fortunate to be advised by Prof. <a href="https://siebelschool.illinois.edu/about/people/faculty/klara"> Klara Nahrstedt</a>, and and to work with Prof. <a href="https://minjiazhang.github.io/">Minjia Zhang</a> and Prof. <a href="https://czhai.cs.illinois.edu/">Chengxiang Zhai</a>. Before starting my PhD, I spent wonderful time at UIUC and Shanghai Jiao Tong University.</p>
        <p>I work on <strong>vision language model agents</strong> and <strong>multimodal reasoning</strong>. In 2025, I have been cooking VLMs and LLMs with recipes of multi-turn reinforcement learning fine-tuning and inference-time scaling, to boost self-improvement, reasoning, and memory use. I also investigate VLM interpretability through circuit tracing. </p>
        <p>My <strong>research goal</strong> is to build human-level agents that partner with people: humans set the intent with prompts; agents reason, execute and suggest new possibilities (via recommendations).</p>
        <p>When I‚Äôm not teaching large models, I prototype augmented-reality systems for fun, hoping that, one day, agentic models will power these interactive, human-centered interfaces.</p>
      </div>

      <div class="foot-space"></div>

      <!-- Industry Experience (independent toggles) -->
      <section class="research">
        <h2>Industry Experience</h2>
        <div class="view-toggle" style="margin:.4rem 0 .2rem 0">
          <button class="toggle-link active" type="button" data-target="exp-list" data-view="selected">Selected</button>
          <span class="muted">/</span>
          <button class="toggle-link" type="button" data-target="exp-list" data-view="all">All</button>
        </div>
        <ul class="list" id="exp-list">
          <li class="publication-item" data-selected="true">
            <div class="paper-title">MRS AI, Meta</div>
            <div class="paper-meta">May 2025 ‚Äì Dec 2025 ‚Ä¢ Research Intern, with Shengyi Qian, Xudong Wang, <b>Hanchao Yu</b></div>
            <div class="paper-meta">Bootstrapping Multimodal Reasoning with Self-verification in RL</div>
          </li>
          <li class="publication-item" data-selected="true">
            <div class="paper-title">Video Rec, Meta</div>
            <div class="paper-meta">May 2024 ‚Äì Dec 2024 ‚Ä¢ Research Intern, with Meng Wu, Daming Li, <b>Arthur Zhang</b></div>
            <div class="paper-meta">VLM/LLM for Short Video Recommendations and Retrieval</div>
          </li>
          <li class="publication-item" data-selected="false">
            <div class="paper-title">LLM Research Team, Capital Today</div>
            <div class="paper-meta">May 2023 ‚Äì Aug 2023 ‚Ä¢ Research Intern, with partners</div>
            <div class="paper-meta">Distinct from academic research work, but with researchers from Tsinghua, ETH and Princeton</div>
          </li>
          <li class="publication-item" data-selected="true">
            <div class="paper-title">Xin's Group, Adobe</div>
            <div class="paper-meta">May 2022 ‚Äì Aug 2022 ‚Ä¢ Research Intern, with Zichuan Liu, <b>Xin Lu</b> (both at Bytedance now)</div>
            <div class="paper-meta">Efficient Interactive Segmentation and Matting for Mobile</div>
          </li>
          <li class="publication-item" data-selected="false">
            <div class="paper-title">IOTG, Intel</div>
            <div class="paper-meta">May 2017 ‚Äì Dec 2017 ‚Ä¢ Machine Vision Software Intern, with <b>Wenjie Wang</b></div>
            <div class="paper-meta">On-Camera Object Detection</div>
          </li>
        </ul>
      </section>

      <!-- Research (independent toggles) -->
      <section class="research">
        <h2>Research</h2>
        <div class="select-line muted">(* denotes equal contribution or main contribution)</div>

        <div class="view-toggle" style="margin:.4rem 0 .2rem 0">
          <button class="toggle-link active" type="button" data-target="pub-list" data-view="selected">Selected</button>
          <span class="muted">/</span>
          <button class="toggle-link" type="button" data-target="pub-list" data-view="all">All</button>
        </div>

        <ul class="list" id="pub-list">
          <!-- ===== Selected ===== -->
          <li class="publication-item" data-selected="true">
            <div class="paper-title">Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?</div>
            <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Meitang Li*, Jingcheng Yang*, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Hanchao Yu, Minjia Zhang, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>NeurIPS 2025 Multimodal Algorithmic Reasoning Workshop</strong>, Oral Presentation.</div>
            <div class="links"><a href="https://arxiv.org/pdf/2506.17417">Paper</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</div>
            <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Jingcheng Yang*, Jize Jiang, Meitang Li, Kaizhuo Yan, Zhaoheng Li, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt</div>
            <div class="paper-meta">Preprint.</div>
            <div class="links"><a href="https://arxiv.org/abs/2505.19255">Paper</a> <a href="https://github.com/VTool-R1/VTool-R1">Code</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning</div>
            <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Jize Jiang*, Haozhen Zheng*, Meitang Li, Zhaoheng Li, Beitong Tian, Bo Chen, Yongjoo Park, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>EMNLP 2025 Main</strong>.</div>
            <div class="links"><a href="https://arxiv.org/pdf/2502.20587">Paper</a> <a href="https://github.com/UIUC-MONET/Cache-of-Thoughts">Code</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">Spatio-Temporal LLM: Reasoning about Environments and Actions</div>
            <div class="paper-meta">Haozhen Zheng, Beitong Tian, <strong>Mingyuan Wu</strong>, Zhenggang Tang, Klara Nahrstedt, Alex Schwing</div>
            <div class="paper-meta">Preprint.</div>
            <div class="links"><a href="https://arxiv.org/abs/2507.05258">Paper</a> <a href="https://github.com/zoezheng126/Spatio-Temporal-LLM">Code</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">RecoWorld: Building Simulated Environments for Agentic Recommender Systems</div>
            <div class="paper-meta">MRS AI Teams at Meta</div>
            <div class="paper-meta">Preprint.</div>
            <div class="links"><a href="https://arxiv.org/abs/2509.10397">Paper</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</div>
            <div class="paper-meta">Xinyu Pi*, <strong>Mingyuan Wu*</strong>, Jize Jiang*, Haozhen Zheng*, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</div>
            <div class="paper-meta"><strong>EMNLP 2024 Main</strong>.</div>
            <div class="links"><a href="https://arxiv.org/pdf/2407.18391">Paper</a> <a href="https://github.com/zoezheng126/UOUO">Code</a></div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">TraceNet: Segment One Thing Efficiently</div>
            <div class="paper-meta"><strong>Mingyuan Wu</strong>, Zichuan Liu, Hongpeng Guo, Bo Chen, Xin Lu, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>IEEE MIPR 2025</strong> ‚Äî üèÜ <strong>Best Student Paper Award</strong>.</div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">Anywhere Avatar: 3D Telepresence with Just a Phone and a Laptop</div>
            <div class="paper-meta">Ruifan Ji, <strong>Mingyuan Wu</strong>, Bo Chen, Michael Zink, Ramesh Sitaraman, Jacob Chakareski, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>ACM Multimedia 2025</strong>, Demo Track.</div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">Scene Graph Driven Hybrid Interactive VR Teleconferencing</div>
            <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Ruifan Ji*, Haozhen Zheng, Jiaxi Li, Beitong Tian, Bo Chen, Ruixiao Zhang, Jacob Chakareski, Michael Zink, Ramesh Sitaraman, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>ACM Multimedia 2024</strong>, Demo Track.</div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">miVirtualSeat: A Next Generation Hybrid Telepresence System</div>
            <div class="paper-meta">Klara Nahrstedt, Ramesh Sitaraman, Jacob Chakareski, Michael Zink, <strong>Mingyuan Wu</strong>, Lingdong Wang, Bo Chen, Ruifan Ji, Kuny Lee, John Murray, Simran Singh, et&nbsp;al.</div>
            <div class="paper-meta"><strong>ACM SIGCOMM 2025 EMS workshop</strong> Oral Presentation.</div>
          </li>

          <li class="publication-item" data-selected="true">
            <div class="paper-title">Seaware: Semantic-aware View Prediction System for 360-degree Video Streaming</div>
            <div class="paper-meta">Jounsuk Park, <strong>Mingyuan Wu</strong>, Kuan-Ying Lee, Bo Chen, Klara Nahrstedt, Michael Zink, Ramesh Sitaraman</div>
            <div class="paper-meta"><strong>IEEE ISM 2020</strong> ‚Äî üèÜ <strong>Best Paper Award</strong>.</div>
          </li>

          <!-- ===== All (extras) ===== -->
          <li class="publication-item" data-selected="false">
            <div class="paper-title">AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models</div>
            <div class="paper-meta">Beitong Tian*, Lingzhi Zhao*, Bo Chen, Haozhen Zheng, Jingcheng Yang, <strong>Mingyuan Wu</strong>, Deepak Vasishth, Klara Nahrstedt</div>
            <div class="paper-meta">Preprint.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">ImmerScope: Multi-view Video Aggregation at Edge towards Immersive Content Services</div>
            <div class="paper-meta">Bo Chen, Hongpeng Guo, <strong>Mingyuan Wu</strong>, Zhe Yang, Zhisheng Yan, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>ACM SenSys 2024</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">GaugeTracker: AI-Powered Cost-Effective Analog Gauge Monitoring System</div>
            <div class="paper-meta">Beitong Tian, <strong>Mingyuan Wu</strong>, Ruixiao Zhang, Haozhen Zheng, Bo Chen, Yaohui Wang, Shiv Trivedi, Shanbo Zhang, et&nbsp;al., Klara Nahrstedt</div>
            <div class="paper-meta"><strong>IEEE MIPR 2024</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">Vesper: Learning to Manage Uncertainty in Video Streaming</div>
            <div class="paper-meta">Bo Chen, <strong>Mingyuan Wu</strong>, Hongpeng Guo, Zhisheng Yan, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>ACM MMSys 2024</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">I-Matting: Improved Trimap-Free Image Matting</div>
            <div class="paper-meta">Zichuan Liu*, Ke Wang*, <strong>Mingyuan Wu*</strong>, Lantao Yu, Klara Nahrstedt, Xin Lu</div>
            <div class="paper-meta"><strong>IEEE ICME 2024</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">Interactive Scene Graph Analysis for Future Intelligent Teleconferencing Systems</div>
            <div class="paper-meta"><strong>Mingyuan Wu</strong>, Yuhan Lu, Shiv Trivedi, Bo Chen, Qian Zhou, Lingdong Wang, Simran Singh, Michael Zink, Ramesh Sitaraman, Jacob Chakareski, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">360TripleView: 360-Degree Video View Management System Driven by Convergence Value of Viewing Preferences</div>
            <div class="paper-meta">Qian Zhou, <strong>Mingyuan Wu</strong>, Yinjie Zhang, Michael Zink, Ramesh Sitaraman, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">SAVG360: Saliency-aware Viewport-guidance-enabled 360-video Streaming System</div>
            <div class="paper-meta">Yinjie Zhang, <strong>Mingyuan Wu</strong>, Beitong Tian, Jiaxi Li, Bo Chen, Qian Zhou, Klara Nahrstedt</div>
            <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
          </li>

          <li class="publication-item" data-selected="false">
            <div class="paper-title">Video 360 Content Navigation for Mobile HMD Devices</div>
            <div class="paper-meta">Jounsuk Park, <strong>Mingyuan Wu</strong>, Eric Lee, Klara Nahrstedt, Yash Shah, Arielle Rosenthal, John Murray, Kevin Spiteri, Michael Zink, Ramesh Sitaraman</div>
            <div class="paper-meta"><strong>ACM Multimedia 2021</strong>.</div>
          </li>

          <!-- Also contributed -->
          <li class="publication-item" data-selected="false">
            <div class="paper-title">AnyLoc: Energy-efficient Visual Localization in Dynamic and Large-Scale Scenes without Pain</div>
            <div class="paper-meta">Beitong Tian, Jingcheng Yang, Jionghao Wei, Zhenggang Tang, Yuheng Wang, Hongwen Xiao, Haozhen Zheng, <strong>Mingyuan Wu</strong>, Shiv Trivedi, Klara Nahrstedt</div>
            <div class="paper-meta">Preprint.</div>
          </li>
        </ul>
      </section>

      <div class="foot-space"></div>

      <section class="events">
        <h2>Events</h2>
        <ul class="list">
          <li><span class="date">Date</span> ‚Äî <a href="#">Invited Talk</a> at TBD Institute.</li>
        </ul>
      </section>

      <div class="foot-space"></div>

      <section class="more">
        <p class="muted"><a href="#">Earlier events‚Ä¶</a></p>
      </section>

      <section class="acknowledgements">
        <h2>Acknowledgements</h2>
        <p>
          This simple website is built from <a href="https://www.jiayipan.com" rel="noopener">Jiayi Pan's website</a> and Codex.
        </p>
      </section>
    </main>
  </div>

  <!-- Script: independent toggles + footnotes -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // Initialize each list to "Selected"
      document.querySelectorAll('ul.list[id]').forEach(ul => {
        toggleListView(ul.id, 'selected');
      });

      // Toggle behavior per section (independent)
      document.querySelectorAll('.view-toggle').forEach(group => {
        group.addEventListener('click', (e) => {
          const btn = e.target.closest('.toggle-link');
          if (!btn) return;
          const listId = btn.dataset.target;   // e.g., "exp-list" or "pub-list"
          const view = btn.dataset.view;       // "selected" or "all"
          toggleListView(listId, view);

          // Active state scoped to this group
          group.querySelectorAll('.toggle-link').forEach(b =>
            b.classList.toggle('active', b === btn)
          );
        });
      });

      // Optional: click-to-expand (ignored when clicking links)
      document.querySelectorAll('.publication-item').forEach(item => {
        item.addEventListener('click', (e) => {
          if (e.target.tagName === 'A' || e.target.closest('a')) return;
          item.classList.toggle('expanded');
        });
      });

      setupFootnotes();
    });

    function toggleListView(listId, viewType){
      const list = document.getElementById(listId);
      if (!list) return;
      list.querySelectorAll('.publication-item').forEach(item => {
        if (viewType === 'selected') {
          item.style.display = (item.getAttribute('data-selected') === 'true') ? 'list-item' : 'none';
        } else {
          item.style.display = 'list-item';
        }
      });
    }

    // Footnote popup logic
    function setupFootnotes(){
      document.querySelectorAll('.footnote-trigger').forEach(trigger => {
        trigger.addEventListener('click', function(e){
          e.preventDefault(); e.stopPropagation();
          // Remove any existing popups
          document.querySelectorAll('.footnote-popup').forEach(p => p.remove());
          const content = this.getAttribute('data-footnote-content') || '';
          const popup = document.createElement('div');
          popup.className = 'footnote-popup';
          popup.innerHTML = '<div class="footnote-content">'+content+'<button class="footnote-close" aria-label="Close">‚úï</button></div>';
          document.body.appendChild(popup);
          // Position near trigger
          const rect = this.getBoundingClientRect();
          const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
          const scrollLeft = window.pageXOffset || document.documentElement.scrollLeft;
          const h = popup.offsetHeight || 140;
          const below = rect.bottom + h < window.innerHeight;
          popup.style.top = (below ? rect.bottom + scrollTop + 5 : rect.top + scrollTop - h - 10) + 'px';
          popup.style.left = (rect.left + scrollLeft) + 'px';
          requestAnimationFrame(()=> popup.classList.add('active'));
          const close = ()=>{ popup.classList.remove('active'); setTimeout(()=>popup.remove(), 150); document.removeEventListener('click', outside); };
          popup.querySelector('.footnote-close').addEventListener('click', close);
          const outside = (evt)=>{ if(!popup.contains(evt.target) && evt.target !== trigger) close(); };
          setTimeout(()=> document.addEventListener('click', outside), 20);
        });
      });
    }
  </script>
</body>
</html>
