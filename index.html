<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Mingyuan Wu Personal Website</title>
  <style>
    :root{
      --bg: #fbf6ed;
      --paper: #fffaf1;
      --ink: #1f2328;
      --muted: #6b7280;
      --accent: #0a58ca;
      --rule: #e8e0d1;
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family: ui-serif, Georgia, Cambria, "Times New Roman", Times, serif;
      color:var(--ink);
      background:var(--bg);
      line-height:1.55;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    .page{
      max-width:1100px;
      margin:0 auto;
      padding:24px 20px 80px;
      display:grid;
      grid-template-columns: 280px 1fr;
      grid-gap: 32px;
    }
    @media (max-width: 860px){
      .page{grid-template-columns:1fr}
      .sidebar{position:static; top:auto}
    }
    .card{
      background:var(--paper);
      border:1px solid var(--rule);
      border-radius:14px;
      box-shadow: 0 1px 0 rgba(0,0,0,.02), 0 6px 20px rgba(0,0,0,.04);
    }
    .sidebar{
      position:sticky;
      top:16px;
      padding:18px;
    }
    .profile{
      display:flex;
      flex-direction:column;
      align-items:center;
      gap:14px;
    }
    .avatar{
      width:140px;height:140px;border-radius:14px;
      object-fit:cover;
      border:1px solid var(--rule);
      background:#f1eadf;
      display:block;
    }
    .name{
      font-weight:700;
      font-size:1.4rem;
    }
    .name small{display:block;color:var(--muted);font-size:.95rem;margin-top:2px}
    .links{
      display:flex;flex-wrap:wrap;gap:10px 14px;justify-content:center;margin-top:6px;
      font-size:0.95rem;
    }
    .research #pub-list .publication-item .links{
      display: flex;            /* keeps multiple links on one line */
      gap: .5rem;
      justify-content: flex-start; /* LEFT align */
      text-align: left;         /* overrides any parent text-align:center */
      margin-left: 0;
    }
    .links a{position:relative}
    .links a:after{
      content:"";position:absolute;left:0;bottom:-1px;width:100%;height:1px;background:currentColor;opacity:.15;
      transform:scaleX(0);transform-origin:left;transition:transform .2s ease;
    }
    .links a:hover:after{transform:scaleX(1)}
    .anon-feedback{display:block;text-align:center;margin-top:2px}
    .content{
      padding:22px 26px;
    }
    h1,h2{
      margin: 10px 0 12px;
    }
    h2{
      font-size:1.15rem;
      letter-spacing:.02em;
      text-transform:uppercase;
      font-weight:800;
      border-bottom:1px solid var(--rule);
      padding-bottom:6px;
    }
    .hello h1{
      font-size:1.6rem;
      display:flex;align-items:center;gap:.55rem;
    }
    .hello p{margin: 10px 0}
    .muted{color:var(--muted)}
    .select-line{
      font-size:.95rem;color:var(--muted);
    }
    .list{
      list-style:none;margin:0;padding:0;
    }
    .list li{
      padding:10px 0 12px;
      border-bottom:1px dashed var(--rule);
    }
    .paper-title{
      font-weight:700;
    }
    .paper-meta{
      color:var(--muted);
      font-size:.96rem;
      margin-top:2px;
    }
    .emoji{font-size:1.1em}
    .events .date{
      font-weight:700;
    }
    .foot-space{height:16px}
    .chip{
      display:inline-block;
      font-size:.8rem;
      padding:.1rem .45rem;
      border:1px solid var(--rule);
      border-radius:999px;
      background:#fff;
      vertical-align:baseline;
      margin-left:.4rem;
      color:var(--muted);
    }
    .topbar{
      display:flex;justify-content:flex-end;gap:10px;margin-bottom:10px;color:var(--muted);font-size:.92rem;
    }

    /* === Additions: footnotes & selected-publications UI (no content changes) === */
    .view-toggle{display:inline-flex;gap:.35rem;align-items:center;font-size:.95rem}
    .toggle-link{background:none;border:0;padding:0 .1rem;font:inherit;cursor:pointer;color:var(--accent)}
    .toggle-link:hover{text-decoration:underline}
    .toggle-link.active{font-weight:700;text-decoration:underline}

    .footnote-trigger{cursor:pointer}
    .footnote-popup{position:absolute;z-index:9999;max-width:320px;background:var(--paper);border:1px solid var(--rule);border-radius:10px;box-shadow:0 10px 25px rgba(0,0,0,.12);opacity:0;transform:translateY(-4px);transition:opacity .18s ease, transform .18s ease}
    .footnote-popup.active{opacity:1;transform:translateY(0)}
    .footnote-content{padding:10px 12px;position:relative}
    .footnote-close{position:absolute;top:6px;right:6px;background:transparent;border:0;font-size:1rem;cursor:pointer}

    /* Optional: expand details on mobile when item has .expanded */
    .publication-item.expanded .details{display:block}




  </style>
</head>
<body>
  <div class="page">
    <!-- Left: Sidebar -->
    <aside class="sidebar card">
      <div class="profile">
        <!-- Embedded SVG placeholder; replace with your own image -->
        <img class="avatar" alt="Pic of Mingyuan Wu"
             src="assets/image_mingyuan.jpg"
             width="1000" height="1000" loading="lazy" decoding="async">
        <div class="name">Mingyuan Wu</div>
        <div class="name"><small>明远</small></div>
        <div class="links">
          <a href="https://x.com/MingyuanWu4" rel="noopener">X/Twitter</a>
          <a href="https://github.com/Mingyuan1997" rel="noopener">GitHub</a>
          <a href="https://scholar.google.com/citations?user=kp3PK7IAAAAJ&hl=en" rel="noopener">Google Scholar</a>
          <a href="mw34@illinois.edu" rel="noopener" class="footnote-trigger" data-footnote-content="mw34@illinois.edu">Email</a>
        </div>
        <div class="name">Under Construction</div>
      </div>
    </aside>

    <!-- Right: Main content -->
    <main class="content card">
      <div class="hello">
        <h2>Some Random Stuffs<span class="emoji">👋</span></h1>
        <p>I am Mingyuan Wu, a part-time researcher at <a href="https://ai.meta.com/">Meta</a>. I am currently a final-year PhD candidate in Computer Science at <a href="https://illinois.edu/"> University of Illinois, Urbana Champaign</a>. I am fortunate to be advised by Prof. <a href="https://siebelschool.illinois.edu/about/people/faculty/klara"> Klara Nahrstedt</a>, and and to work with Prof. <a href="https://minjiazhang.github.io/">Minjia Zhang</a> and Prof. <a href="https://czhai.cs.illinois.edu/">Chengxiang Zhai</a>. Before starting my PhD, I spent wonderful time at UIUC and Shanghai Jiao Tong University.</p>
        <p>I work on <strong>vision language model agents</strong> and <strong>multimodal reasoning</strong>. In 2025, I have been cooking VLMs and LLMs with recipes of multi-turn reinforcement learning fine-tuning and inference-time scaling, to boost self-improvement, reasoning, and memory use. </p>
        <p>My <strong>research goal</strong> is to build human-level agents that partner with people: humans set the intent with prompts; agents reason, execute and suggest new possibilities (via recommendations).</p>
        <p>When I’m not teaching large models, I prototype augmented-reality systems for fun, hoping that, one day, agentic models will power these interactive, human-centered interfaces.</p>
      </div>

      <div class="foot-space"></div>
      
      <section class="research">
        <h2>Industry Experience</h2>
        <ul class="list">
          <li class="publication-item" data-selected="true">
            <div class="paper-title">MRS AI, Meta</div>
            <div class="paper-meta">May 2025 – Dec 2025 • Research Intern, with Shengyi Qian, Xudong Wang, <b>Hanchao Yu</b></div>
            <div class="paper-meta">Bootstrapping Multimodal Reasoning with Self-verification in RL</div>
          </li>
          <li class="publication-item" data-selected="true">
            <div class="paper-title">Video Rec, Meta</div>
            <div class="paper-meta">May 2024 – Dec 2024 • Research Intern, with Meng Wu, Daming Li, <b>Arthur Zhang</b></div>
            <div class="paper-meta">VLM/LLM for Short Video Recommendations and Retrieval</div>
          </li>
          <li class="publication-item" data-selected="true">
            <div class="paper-title">LLM Research Team, Capital Today</div>
            <div class="paper-meta">May 2023 – Aug 2023 • Research Intern, with partners</div>
            <div class="paper-meta">Distinct from academic research work, but with researchers from Tsinghua, ETH and Princeton</div>
          </li>        
          <li class="publication-item" data-selected="true">
            <div class="paper-title">Xin's Group, Adobe</div>
            <div class="paper-meta">May 2022 – Aug 2022 • Research Intern, with Zichuan Liu, <b>Xin Lu</b> (both at Bytedance now)</div>
            <div class="paper-meta">Efficient Interactive Segmentation and Matting for Mobile</div>
          </li>
          <li class="publication-item" data-selected="true">
            <div class="paper-title">IOTG, Intel</div>
            <div class="paper-meta">May 2017 – Dec 2017 • Machine Vision Software Intern, with <b>Wenjie Wang</b></div>
            <div class="paper-meta">On-Camera Object Detection</div>
          </li>
        </ul>
      </section>
      
 <section class="research">
  <h2>Research</h2>
  <div class="select-line muted">(* denotes equal contribution)</div>

  <div class="view-toggle" style="margin:.4rem 0 .2rem 0">
    <button id="selected-toggle" class="toggle-link active" type="button">Selected</button>
    <span class="muted">/</span>
    <button id="all-toggle" class="toggle-link" type="button">All</button>
  </div>

  <ul class="list" id="pub-list">
    <!-- ===== Selected ===== -->
    <li class="publication-item" data-selected="true">
      <div class="paper-title">Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?</div>
      <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Meitang Li*, Jingcheng Yang*, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Hanchao Yu, Minjia Zhang, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>NeurIPS 2025 Multimodal Algorithmic Reasoning Workshop</strong>, Oral Presentation.</div>
      <div class="links"><a href="https://arxiv.org/pdf/2506.17417">Paper</a></div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</div>
      <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Jingcheng Yang*, Jize Jiang, Meitang Li, Kaizhuo Yan, Zhaoheng Li, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt</div>
      <div class="paper-meta">Preprint.</div>
      <div class="links"><a href="https://arxiv.org/abs/2505.19255">Paper</a> <a href="https://github.com/VTool-R1/VTool-R1">Code</a></div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning</div>
      <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Jize Jiang*, Haozhen Zheng*, Meitang Li, Zhaoheng Li, Beitong Tian, Bo Chen, Yongjoo Park, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>EMNLP 2025 Main</strong>.</div>
      <div class="links"><a href="https://arxiv.org/pdf/2502.20587">Paper</a> <a href="https://github.com/UIUC-MONET/Cache-of-Thoughts">Code</a></div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">Spatio-Temporal LLM: Reasoning about Environments and Actions</div>
      <div class="paper-meta">Haozhen Zheng, Beitong Tian, <strong>Mingyuan Wu</strong>, Zhenggang Tang, Klara Nahrstedt, Alex Schwing</div>
      <div class="paper-meta">Preprint.</div>
      <div class="links"><a href="https://arxiv.org/abs/2507.05258">Paper</a> <a href="https://github.com/zoezheng126/Spatio-Temporal-LLM">Code</a></div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">RecoWorld: Building Simulated Environments for Agentic Recommender Systems</div>
      <div class="paper-meta">MRS AI Teams at Meta</div>
      <div class="paper-meta">Preprint.</div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">TraceNet: Segment One Thing Efficiently</div>
      <div class="paper-meta"><strong>Mingyuan Wu</strong>, Zichuan Liu, Hongpeng Guo, Bo Chen, Xin Lu, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>IEEE MIPR 2025</strong> — 🏆 <strong>Best Student Paper Award</strong>.</div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">Anywhere Avatar: 3D Telepresence with Just a Phone and a Laptop</div>
      <div class="paper-meta">Ruifan Ji, <strong>Mingyuan Wu</strong>, Bo Chen, Michael Zink, Ramesh Sitaraman, Jacob Chakareski, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>ACM Multimedia 2025</strong>, Demo Track.</div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">Scene Graph Driven Hybrid Interactive VR Teleconferencing</div>
      <div class="paper-meta"><strong>Mingyuan Wu*</strong>, Ruifan Ji*, Haozhen Zheng, Jiaxi Li, Beitong Tian, Bo Chen, Ruixiao Zhang, Jacob Chakareski, Michael Zink, Ramesh Sitaraman, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>ACM Multimedia 2024</strong>, Demo Track.</div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</div>
      <div class="paper-meta">Xinyu Pi*, <strong>Mingyuan Wu*</strong>, Jize Jiang*, Haozhen Zheng*, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</div>
      <div class="paper-meta"><strong>EMNLP 2024 Main</strong>.</div>
      <div class="links"><a href="https://arxiv.org/pdf/2407.18391">Paper</a> <a href="https://github.com/zoezheng126/UOUO">Code</a></div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">miVirtualSeat: A Next Generation Hybrid Telepresence System</div>
      <div class="paper-meta">Klara Nahrstedt, Ramesh Sitaraman, Jacob Chakareski, Michael Zink, <strong>Mingyuan Wu</strong>, Lingdong Wang, Bo Chen, Ruifan Ji, Kuny Lee, John Murray, Simran Singh, et&nbsp;al.</div>
      <div class="paper-meta"><strong>ACM SIGCOMM 2025 EMS workshop</strong> Oral Presentation.</div>
    </li>

    <li class="publication-item" data-selected="true">
      <div class="paper-title">Seaware: Semantic-aware View Prediction System for 360-degree Video Streaming</div>
      <div class="paper-meta">Jounsuk Park, <strong>Mingyuan Wu</strong>, Kuan-Ying Lee, Bo Chen, Klara Nahrstedt, Michael Zink, Ramesh Sitaraman</div>
      <div class="paper-meta"><strong>IEEE ISM 2020</strong> — 🏆 <strong>Best Paper Award</strong>.</div>
    </li>

    <!-- ===== All publications (includes the selected above) ===== -->
    <li class="publication-item" data-selected="false">
      <div class="paper-title">AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models</div>
      <div class="paper-meta">Beitong Tian*, Lingzhi Zhao*, Bo Chen, Haozhen Zheng, Jingcheng Yang, <strong>Mingyuan Wu</strong>, Deepak Vasishth, Klara Nahrstedt</div>
      <div class="paper-meta">Preprint.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">ImmerScope: Multi-view Video Aggregation at Edge towards Immersive Content Services</div>
      <div class="paper-meta">Bo Chen, Hongpeng Guo, <strong>Mingyuan Wu</strong>, Zhe Yang, Zhisheng Yan, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>ACM SenSys 2024</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">GaugeTracker: AI-Powered Cost-Effective Analog Gauge Monitoring System</div>
      <div class="paper-meta">Beitong Tian, <strong>Mingyuan Wu</strong>, Ruixiao Zhang, Haozhen Zheng, Bo Chen, Yaohui Wang, Shiv Trivedi, Shanbo Zhang, et&nbsp;al., Klara Nahrstedt</div>
      <div class="paper-meta"><strong>IEEE MIPR 2024</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">Vesper: Learning to Manage Uncertainty in Video Streaming</div>
      <div class="paper-meta">Bo Chen, <strong>Mingyuan Wu</strong>, Hongpeng Guo, Zhisheng Yan, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>ACM MMSys 2024</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">I-Matting: Improved Trimap-Free Image Matting</div>
      <div class="paper-meta">Zichuan Liu*, Ke Wang*, <strong>Mingyuan Wu*</strong>, Lantao Yu, Klara Nahrstedt, Xin Lu</div>
      <div class="paper-meta"><strong>IEEE ICME 2024</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">Interactive Scene Graph Analysis for Future Intelligent Teleconferencing Systems</div>
      <div class="paper-meta"><strong>Mingyuan Wu</strong>, Yuhan Lu, Shiv Trivedi, Bo Chen, Qian Zhou, Lingdong Wang, Simran Singh, Michael Zink, Ramesh Sitaraman, Jacob Chakareski, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">360TripleView: 360-Degree Video View Management System Driven by Convergence Value of Viewing Preferences</div>
      <div class="paper-meta">Qian Zhou, <strong>Mingyuan Wu</strong>, Yinjie Zhang, Michael Zink, Ramesh Sitaraman, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">SAVG360: Saliency-aware Viewport-guidance-enabled 360-video Streaming System</div>
      <div class="paper-meta">Yinjie Zhang, <strong>Mingyuan Wu</strong>, Beitong Tian, Jiaxi Li, Bo Chen, Qian Zhou, Klara Nahrstedt</div>
      <div class="paper-meta"><strong>IEEE ISM 2023</strong>.</div>
    </li>

    <li class="publication-item" data-selected="false">
      <div class="paper-title">Video 360 Content Navigation for Mobile HMD Devices</div>
      <div class="paper-meta">Jounsuk Park, <strong>Mingyuan Wu</strong>, Eric Lee, Klara Nahrstedt, Yash Shah, Arielle Rosenthal, John Murray, Kevin Spiteri, Michael Zink, Ramesh Sitaraman</div>
      <div class="paper-meta"><strong>ACM Multimedia 2021</strong>.</div>
    </li>


    <!-- ===== Also contributed to ===== -->
    <li class="publication-item" data-selected="false">
      <div class="paper-title">AnyLoc: Energy-efficient Visual Localization in Dynamic and Large-Scale Scenes without Pain</div>
      <div class="paper-meta">Beitong Tian, Jingcheng Yang, Jionghao Wei, Zhenggang Tang, Yuheng Wang, Hongwen Xiao, Haozhen Zheng, <strong>Mingyuan Wu</strong>, Shiv Trivedi, Klara Nahrstedt</div>
      <div class="paper-meta">Preprint.</div>
    </li>
  </ul>
</section>


      <div class="foot-space"></div>

      <section class="events">
        <h2>Events</h2>
        <ul class="list">
          <li><span class="date">Date</span> — <a href="#">Invited Talk</a> at TBD Institute.</li>
        </ul>
      </section>

      <div class="foot-space"></div>

      <section class="more">
        <p class="muted"><a href="#">Earlier events…</a></p>
      </section>
      <section class="acknowledgements">
        <h2>Acknowledgements</h2>
        <p>
          This simple website is built from <a href="https://www.jiayipan.com" rel="noopener">Jiayi Pan's website</a> and Codex,
        </p>
      </section>
    </main>
  </div>

  <!-- Lightweight JS added; does not alter existing content -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // Publications: default to Selected
      showSelectedPublications();

      // Click-to-expand (mobile-friendly); ignores clicks on links
      document.querySelectorAll('.publication-item').forEach(pub => {
        pub.addEventListener('click', (e) => {
          if (e.target.tagName === 'A' || e.target.closest('a')) return;
          pub.classList.toggle('expanded');
        });
      });

      // Toggle buttons
      const selBtn = document.getElementById('selected-toggle');
      const allBtn = document.getElementById('all-toggle');
      if (selBtn && allBtn){
        selBtn.addEventListener('click', () => togglePublicationView('selected'));
        allBtn.addEventListener('click', () => togglePublicationView('all'));
      }

      // Footnote‑style popups (e.g., for Email)
      setupFootnotes();
    });

    function togglePublicationView(viewType){
      const pubs = document.querySelectorAll('#pub-list .publication-item');
      pubs.forEach(pub => {
        if (viewType === 'selected'){
          pub.style.display = (pub.getAttribute('data-selected') === 'true') ? 'list-item' : 'none';
        } else {
          pub.style.display = 'list-item';
        }
      });
      const selBtn = document.getElementById('selected-toggle');
      const allBtn = document.getElementById('all-toggle');
      if (selBtn && allBtn){
        selBtn.classList.toggle('active', viewType === 'selected');
        allBtn.classList.toggle('active', viewType === 'all');
      }
    }
    function showSelectedPublications(){ togglePublicationView('selected'); }

    // Footnote popup logic (borrowed pattern, minimal and self‑contained)
    function setupFootnotes(){
      document.querySelectorAll('.footnote-trigger').forEach(trigger => {
        trigger.addEventListener('click', function(e){
          e.preventDefault(); e.stopPropagation();
          // Remove any existing popups
          document.querySelectorAll('.footnote-popup').forEach(p => p.remove());
          const content = this.getAttribute('data-footnote-content') || '';
          const popup = document.createElement('div');
          popup.className = 'footnote-popup';
          popup.innerHTML = '<div class="footnote-content">'+content+'<button class="footnote-close" aria-label="Close">✕</button></div>';
          document.body.appendChild(popup);
          // Position near trigger
          const rect = this.getBoundingClientRect();
          const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
          const scrollLeft = window.pageXOffset || document.documentElement.scrollLeft;
          const h = popup.offsetHeight || 140;
          const below = rect.bottom + h < window.innerHeight;
          popup.style.top = (below ? rect.bottom + scrollTop + 5 : rect.top + scrollTop - h - 10) + 'px';
          popup.style.left = (rect.left + scrollLeft) + 'px';
          requestAnimationFrame(()=> popup.classList.add('active'));
          const close = ()=>{ popup.classList.remove('active'); setTimeout(()=>popup.remove(), 150); document.removeEventListener('click', outside); };
          popup.querySelector('.footnote-close').addEventListener('click', close);
          const outside = (evt)=>{ if(!popup.contains(evt.target) && evt.target !== trigger) close(); };
          setTimeout(()=> document.addEventListener('click', outside), 20);
        });
      });
    }
  </script>
</body>
</html>
